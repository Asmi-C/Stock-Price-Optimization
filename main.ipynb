{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe41ff91",
   "metadata": {},
   "source": [
    "### Importing the required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545bf9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from alpha_vantage.timeseries import TimeSeries\n",
    "from alpha_vantage.foreignexchange import ForeignExchange\n",
    "from alpha_vantage.commodities import Commodities\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from keras import Input\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from keras.models import load_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6427ad95",
   "metadata": {},
   "source": [
    "### Alpha Vantage Setup (Trading API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9305c9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# api_key = \"GLVZ9GJN4IW7GRUB\"\n",
    "api_key = \"K757OWEW19L34ML9\"\n",
    "# symbols = ['AAPL', 'MSFT', 'GOOGL', 'TSLA', 'AMZN']  # Multiple stocks for analysis\n",
    "ts = TimeSeries(key=api_key, output_format=\"pandas\")\n",
    "fx = ForeignExchange(key=api_key, output_format=\"pandas\")\n",
    "\n",
    "assets = [\n",
    "    {\"symbol\": \"AAPL\", \"type\": \"stock\"},\n",
    "    {\"symbol\": \"MSFT\", \"type\": \"stock\"},\n",
    "    {\"symbol\": \"TSLA\", \"type\": \"stock\"},\n",
    "    {\"symbol\": \"AMZN\", \"type\": \"stock\"},\n",
    "    {\"symbol\": \"EURUSD\", \"type\": \"forex\", \"from_symbol\": \"EUR\", \"to_symbol\": \"USD\"},\n",
    "    {\"symbol\": \"USDJPY\", \"type\": \"forex\", \"from_symbol\": \"USD\", \"to_symbol\": \"JPY\"},\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a59d6f8",
   "metadata": {},
   "source": [
    "### Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2366f85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MODEL_DIR = Path(\"models/lstm\")\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "LSTM_WINDOW = 60  # single source of truth\n",
    "BACKTEST_YEARS = 2  # enforce last-2-years window for the backtest\n",
    "\n",
    "results = {}\n",
    "stock_stats = []\n",
    "GLOBAL_START = datetime(2010, 6, 29)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b097498a",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a60e0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = data.copy()\n",
    "\n",
    "    # drop accidental index columns from prior saves\n",
    "    drop_cols = [c for c in df.columns if str(c).lower().startswith(\"unnamed\")]\n",
    "    if drop_cols:\n",
    "        df = df.drop(columns=drop_cols)\n",
    "\n",
    "    cols_lower = [c.lower() for c in df.columns]\n",
    "\n",
    "    # Case 1: Stock/forex style (AV or your yfinance-normalized CSVs)\n",
    "    if \"1. open\" in df.columns:\n",
    "        df = df.rename(\n",
    "            columns={\n",
    "                \"1. open\": \"open\",\n",
    "                \"2. high\": \"high\",\n",
    "                \"3. low\": \"low\",\n",
    "                \"4. close\": \"close\",\n",
    "                \"5. volume\": \"volume\",\n",
    "            }\n",
    "        )[[\"open\", \"high\", \"low\", \"close\", \"volume\"]]\n",
    "\n",
    "    # Case 2: Commodity style -> timestamp,value (raw AV commodity pull)\n",
    "    elif \"timestamp\" in cols_lower and \"value\" in cols_lower:\n",
    "        rename_map = {}\n",
    "        for c in df.columns:\n",
    "            cl = c.lower()\n",
    "            if cl == \"timestamp\":\n",
    "                rename_map[c] = \"date\"\n",
    "            elif cl == \"value\":\n",
    "                rename_map[c] = \"close\"\n",
    "        df = df.rename(columns=rename_map)\n",
    "\n",
    "        df[\"open\"] = df[\"close\"]\n",
    "        df[\"high\"] = df[\"close\"]\n",
    "        df[\"low\"] = df[\"close\"]\n",
    "        df[\"volume\"] = 0\n",
    "        df = df[[\"open\", \"high\", \"low\", \"close\", \"volume\"]]\n",
    "\n",
    "    # Case 3: Generic single-value CSV with 'value'\n",
    "    elif \"value\" in df.columns:\n",
    "        df = df.rename(columns={\"value\": \"close\"})\n",
    "        df[\"open\"] = df[\"close\"]\n",
    "        df[\"high\"] = df[\"close\"]\n",
    "        df[\"low\"] = df[\"close\"]\n",
    "        df[\"volume\"] = 0\n",
    "        df = df[[\"open\", \"high\", \"low\", \"close\", \"volume\"]]\n",
    "\n",
    "    else:\n",
    "        # Fallback: ensure 'close' exists\n",
    "        if \"close\" not in df.columns:\n",
    "            num_cols = df.select_dtypes(include=[np.number]).columns\n",
    "            if len(num_cols) == 0:\n",
    "                raise ValueError(\n",
    "                    f\"Unknown data format, cannot infer 'close' from: {df.columns.tolist()}\"\n",
    "                )\n",
    "            df[\"close\"] = df[num_cols[-1]]\n",
    "        df[\"open\"] = df.get(\"open\", df[\"close\"])\n",
    "        df[\"high\"] = df.get(\"high\", df[\"close\"])\n",
    "        df[\"low\"] = df.get(\"low\", df[\"close\"])\n",
    "        df[\"volume\"] = df.get(\"volume\", 0)\n",
    "        df = df[[\"open\", \"high\", \"low\", \"close\", \"volume\"]]\n",
    "\n",
    "    # Ensure oldest → newest for modeling\n",
    "    df = df[::-1].reset_index(drop=True)\n",
    "\n",
    "    # ✅ enforce global start date for all assets\n",
    "    if \"date\" in df.columns:\n",
    "        df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "        df = df[df[\"date\"] >= GLOBAL_START]\n",
    "        df = df.reset_index(drop=True)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48bbeda",
   "metadata": {},
   "source": [
    "### Load the locally stored data or Fetch the data using the API\n",
    "Better to load the data as the API has a limit on the number of requests per day. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9072f99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_or_fetch_data(asset, api_key, directory=\"asset_data\"):\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    symbol = asset[\"symbol\"]\n",
    "    filepath = os.path.join(directory, f\"{symbol}.csv\")\n",
    "\n",
    "    # Symbols you manage locally via yfinance (do NOT refetch/overwrite)\n",
    "    LOCAL_ONLY_SYMBOLS = {\"COPPER\", \"NATURAL_GAS\"}\n",
    "\n",
    "    # If a cached CSV exists, always use it\n",
    "    if os.path.exists(filepath):\n",
    "        print(f\"Loading cached data for {symbol}\")\n",
    "        df = pd.read_csv(filepath)\n",
    "        df = prepare_data(df)\n",
    "        return df\n",
    "\n",
    "    # If it's a local-only symbol and no file exists yet, fail loudly with guidance\n",
    "    if symbol in LOCAL_ONLY_SYMBOLS:\n",
    "        raise FileNotFoundError(\n",
    "            f\"{symbol}.csv not found in '{directory}'. \"\n",
    "            f\"Generate it first with the yfinance script, saved as \"\n",
    "            f\"'date,1. open,2. high,3. low,4. close,5. volume'.\"\n",
    "        )\n",
    "\n",
    "    # Otherwise, fetch from Alpha Vantage for stocks/forex\n",
    "    print(f\"Fetching data for {symbol} from Alpha Vantage...\")\n",
    "\n",
    "    if asset[\"type\"] == \"stock\":\n",
    "        ts = TimeSeries(key=api_key, output_format=\"pandas\")\n",
    "        df_raw, _ = ts.get_daily(symbol=symbol, outputsize=\"full\")\n",
    "        # print first 5 rows with the headers\n",
    "        # print(df_raw.head())\n",
    "\n",
    "    elif asset[\"type\"] == \"forex\":\n",
    "        fx = ForeignExchange(key=api_key, output_format=\"pandas\")\n",
    "        df_raw, _ = fx.get_currency_exchange_daily(\n",
    "            from_symbol=asset[\"from_symbol\"],\n",
    "            to_symbol=asset[\"to_symbol\"],\n",
    "            outputsize=\"full\",\n",
    "        )\n",
    "        # AV doesn't provide FX volume\n",
    "        df_raw[\"5. volume\"] = 0\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown asset type {asset['type']}\")\n",
    "\n",
    "    # Make sure we're working on a fresh copy\n",
    "    df_raw = df_raw.copy()\n",
    "    # Turn the index into a column\n",
    "    df_raw = df_raw.reset_index()\n",
    "    # Rename the index column explicitly\n",
    "    df_raw = df_raw.rename(columns={\"index\": \"date\"})\n",
    "\n",
    "    # print(df_raw.head())\n",
    "\n",
    "    # # ✅ Save cleanly (no index) so reloads don't create duplicate 'date' columns\n",
    "    df_raw.to_csv(filepath, index=False)\n",
    "    print(f\"Saved {symbol} data to {filepath}\")\n",
    "\n",
    "    # Normalize columns and order\n",
    "    df = prepare_data(df_raw)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb18e60",
   "metadata": {},
   "source": [
    "### Helper Functions for Data and Model Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cc27c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_two_years(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Keep only the last BACKTEST_YEARS of data by 'date' column if present.\n",
    "    Assumes df is oldest→newest after prepare_data().\n",
    "    \"\"\"\n",
    "    if \"date\" in df.columns:\n",
    "        end = pd.to_datetime(df[\"date\"]).max()\n",
    "        start = end - pd.Timedelta(days=365 * BACKTEST_YEARS)\n",
    "        out = df[pd.to_datetime(df[\"date\"]).between(start, end)].reset_index(drop=True)\n",
    "        return out\n",
    "    return df\n",
    "\n",
    "\n",
    "def _artifact_paths(symbol: str):\n",
    "    model_path = MODEL_DIR / f\"{symbol}.keras\"\n",
    "    meta_path = MODEL_DIR / f\"{symbol}_meta.pkl\"\n",
    "    return model_path, meta_path\n",
    "\n",
    "\n",
    "def _save_meta(meta_path: Path, meta: dict):\n",
    "    with open(meta_path, \"wb\") as f:\n",
    "        pickle.dump(meta, f)\n",
    "\n",
    "\n",
    "def _load_meta(meta_path: Path) -> dict:\n",
    "    with open(meta_path, \"rb\") as f:\n",
    "        return pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f1a799",
   "metadata": {},
   "source": [
    "### Technical indicators for XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415de423",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_technical_indicators(df):\n",
    "    if \"close\" not in df.columns:\n",
    "        raise KeyError(\"The 'close' column is missing in the DataFrame.\")\n",
    "\n",
    "    df[\"MA5\"] = df[\"close\"].rolling(window=5).mean()\n",
    "    df[\"MA10\"] = df[\"close\"].rolling(window=10).mean()\n",
    "    df[\"MA20\"] = df[\"close\"].rolling(window=20).mean()\n",
    "    df[\"Return_5\"] = df[\"close\"].pct_change(periods=5)\n",
    "    df[\"Volatility_20\"] = df[\"close\"].rolling(window=20).std()\n",
    "    df[\"RSI\"] = compute_rsi(df[\"close\"], 14)\n",
    "    df[\"MACD\"] = compute_macd(df[\"close\"])\n",
    "    df = df.dropna()\n",
    "    return df\n",
    "\n",
    "\n",
    "def compute_rsi(series, period=14):\n",
    "    delta = series.diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()\n",
    "    rs = gain / loss\n",
    "    return 100 - (100 / (1 + rs))\n",
    "\n",
    "\n",
    "def compute_macd(series, slow=26, fast=12):\n",
    "    ema_fast = series.ewm(span=fast, adjust=False).mean()\n",
    "    ema_slow = series.ewm(span=slow, adjust=False).mean()\n",
    "    return ema_fast - ema_slow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4d7477",
   "metadata": {},
   "source": [
    "### Visualize which features were selected and how important they are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c134041",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_xgboost_feature_importance(model, feature_names, symbol):\n",
    "    importances = model.feature_importances_\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.bar(range(len(feature_names)), importances[indices], align=\"center\")\n",
    "    plt.xticks(\n",
    "        range(len(feature_names)), [feature_names[i] for i in indices], rotation=45\n",
    "    )\n",
    "    plt.title(f\"{symbol} - XGBoost Feature Importances\")\n",
    "    plt.xlabel(\"Features\")\n",
    "    plt.ylabel(\"Importance Score\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b2b2b5",
   "metadata": {},
   "source": [
    "### XGBoost for Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ea33e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def select_features(df, target_col=\"close\", symbol=\"\"):\n",
    "    df = df.select_dtypes(include=[np.number])\n",
    "\n",
    "    # target: future return (use percent change)\n",
    "    y = df[target_col].pct_change().shift(-1).dropna()\n",
    "    df = df.iloc[:-1].reset_index(drop=True)  # Align features to y\n",
    "\n",
    "    X = df.drop(columns=[target_col])\n",
    "    model = XGBRegressor(random_state=42)  # setting a seed for reproducibility\n",
    "    model.fit(X, y)\n",
    "\n",
    "    # plot_xgboost_feature_importance(model, X.columns, symbol)\n",
    "\n",
    "    importances = model.feature_importances_\n",
    "    feature_importance_df = (\n",
    "        pd.DataFrame({\"Feature\": X.columns, \"Importance\": importances})\n",
    "        .sort_values(by=\"Feature\")\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    print(\"\\nFull Feature Importance Ranking:\")\n",
    "    print(feature_importance_df)\n",
    "\n",
    "    top_features = X.columns[np.argsort(model.feature_importances_)][-5:]\n",
    "    X_selected = X[top_features].copy().reset_index(drop=True)\n",
    "    y_target = (\n",
    "        df[target_col].iloc[1:].reset_index(drop=True)\n",
    "    )  # align with prediction target\n",
    "\n",
    "    y_target.name = target_col  # ✅ set the name to 'Close' (important for merging)\n",
    "\n",
    "    return X_selected, y_target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3064a27c",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning using Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de994542",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tune_xgb_hyperparams(X, y, n_iter=25, cv=3, random_state=42):\n",
    "    \"\"\"\n",
    "    Lightweight hyperparam search for XGBRegressor.\n",
    "    Returns: best_estimator_, best_params_\n",
    "    \"\"\"\n",
    "    base = XGBRegressor(\n",
    "        n_estimators=600,\n",
    "        tree_method=\"hist\",  # fast CPU training\n",
    "        random_state=random_state,\n",
    "    )\n",
    "    param_dist = {\n",
    "        \"max_depth\": [3, 4, 5, 6, 7, 8],\n",
    "        \"learning_rate\": [0.01, 0.02, 0.05, 0.08, 0.1],\n",
    "        \"subsample\": [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "        \"colsample_bytree\": [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "        \"min_child_weight\": [1, 3, 5, 7, 10],\n",
    "        \"gamma\": [0, 0.1, 0.2, 0.3],\n",
    "        \"reg_lambda\": [0.0, 0.5, 1.0, 2.0, 5.0],\n",
    "        \"reg_alpha\": [0.0, 0.5, 1.0, 2.0],\n",
    "    }\n",
    "    search = RandomizedSearchCV(\n",
    "        base,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=n_iter,\n",
    "        scoring=\"neg_mean_squared_error\",\n",
    "        cv=cv,\n",
    "        n_jobs=-1,\n",
    "        verbose=0,\n",
    "        random_state=random_state,\n",
    "    )\n",
    "    search.fit(X, y)\n",
    "    return search.best_estimator_, search.best_params_\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a9109d",
   "metadata": {},
   "source": [
    "### LSTM Forecasting (for multivariate time series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950f905f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_forecast_multivariate(\n",
    "    data: pd.DataFrame,\n",
    "    target: str = \"close\",\n",
    "    symbol: str | None = None,\n",
    "    use_cache: bool = True,\n",
    "    force_retrain: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Train (or load) an LSTM for one asset and return (actual, pred) arrays on the test split.\n",
    "    Caches: Keras model + fitted MinMaxScaler + feature schema under models/lstm/.\n",
    "    \"\"\"\n",
    "    assert isinstance(target, str), \"Target must be a string\"\n",
    "    assert target in data.columns, f\"Target column '{target}' not found\"\n",
    "\n",
    "    feature_cols = [c for c in data.columns if c != target]\n",
    "    full_data = data[feature_cols + [target]]  # fixed order\n",
    "    target_index = full_data.columns.get_loc(target)\n",
    "\n",
    "    model_path = meta_path = None\n",
    "    if symbol:\n",
    "        model_path, meta_path = _artifact_paths(symbol)\n",
    "\n",
    "    # ---------- Try cache ----------\n",
    "    if (\n",
    "        use_cache\n",
    "        and not force_retrain\n",
    "        and symbol is not None\n",
    "        and model_path.exists()\n",
    "        and meta_path.exists()\n",
    "    ):\n",
    "        try:\n",
    "            meta = _load_meta(meta_path)\n",
    "            schema_ok = (\n",
    "                meta.get(\"target\") == target\n",
    "                and meta.get(\"feature_cols\") == feature_cols\n",
    "                and meta.get(\"window\") == LSTM_WINDOW\n",
    "            )\n",
    "            if schema_ok:\n",
    "                model = load_model(model_path)\n",
    "                scaler = meta[\"scaler\"]\n",
    "\n",
    "                scaled = scaler.transform(full_data)\n",
    "                X_all, y_all = [], []\n",
    "                for i in range(LSTM_WINDOW, len(scaled)):\n",
    "                    X_all.append(scaled[i - LSTM_WINDOW : i, :-1])\n",
    "                    y_all.append(scaled[i, target_index])\n",
    "                X_all, y_all = np.array(X_all), np.array(y_all)\n",
    "\n",
    "                # ordered time split: last 20% = test\n",
    "                split = int(len(X_all) * 0.8)\n",
    "                X_test, y_test = X_all[split:], y_all[split:]\n",
    "\n",
    "                pred_scaled = model.predict(X_test, verbose=0)\n",
    "                y_test = y_test.reshape(-1, 1)\n",
    "\n",
    "                pred_full = np.zeros((len(pred_scaled), full_data.shape[1]))\n",
    "                actual_full = np.zeros_like(pred_full)\n",
    "                pred_full[:, target_index] = pred_scaled[:, 0]\n",
    "                actual_full[:, target_index] = y_test[:, 0]\n",
    "\n",
    "                # pred = MinMaxScaler().inverse_transform(\n",
    "                #     np.where(\n",
    "                #         np.arange(full_data.shape[1]) == target_index, pred_full, 0\n",
    "                #     )\n",
    "                # )  # not used; see below\n",
    "                # # The above is not correct; inverse needs the *same* scaler. Build properly:\n",
    "                pred = scaler.inverse_transform(pred_full)[:, target_index]\n",
    "                actual = scaler.inverse_transform(actual_full)[:, target_index]\n",
    "                return actual, pred\n",
    "            else:\n",
    "                print(f\"[{symbol}] Cached model schema mismatch → retraining...\")\n",
    "        except Exception as e:\n",
    "            print(f\"[{symbol}] Failed to load cached model: {e}. Retraining...\")\n",
    "\n",
    "    # ---------- Train fresh ----------\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled = scaler.fit_transform(full_data)\n",
    "\n",
    "    X_all, y_all = [], []\n",
    "    for i in range(LSTM_WINDOW, len(scaled)):\n",
    "        X_all.append(scaled[i - LSTM_WINDOW : i, :-1])\n",
    "        y_all.append(scaled[i, target_index])\n",
    "    X_all, y_all = np.array(X_all), np.array(y_all)\n",
    "\n",
    "    # ordered time split: last 20% = test\n",
    "    split = int(len(X_all) * 0.8)\n",
    "    X_train, y_train = X_all[:split], y_all[:split]\n",
    "    X_test, y_test = X_all[split:], y_all[split:]\n",
    "\n",
    "    model = Sequential(\n",
    "        [\n",
    "            Input(shape=(X_all.shape[1], X_all.shape[2])),\n",
    "            LSTM(64, return_sequences=True),\n",
    "            LSTM(32),\n",
    "            Dense(1),\n",
    "        ]\n",
    "    )\n",
    "    model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "    model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0)\n",
    "\n",
    "    pred_scaled = model.predict(X_test, verbose=0)\n",
    "    y_test = y_test.reshape(-1, 1)\n",
    "\n",
    "    pred_full = np.zeros((len(pred_scaled), full_data.shape[1]))\n",
    "    actual_full = np.zeros_like(pred_full)\n",
    "    pred_full[:, target_index] = pred_scaled[:, 0]\n",
    "    actual_full[:, target_index] = y_test[:, 0]\n",
    "\n",
    "    pred = scaler.inverse_transform(pred_full)[:, target_index]\n",
    "    actual = scaler.inverse_transform(actual_full)[:, target_index]\n",
    "\n",
    "    # Save artifacts\n",
    "    if symbol and use_cache:\n",
    "        try:\n",
    "            model.save(model_path)\n",
    "            meta = {\n",
    "                \"scaler\": scaler,\n",
    "                \"feature_cols\": feature_cols,\n",
    "                \"target\": target,\n",
    "                \"window\": LSTM_WINDOW,\n",
    "                \"trained_at\": datetime.utcnow().isoformat(),\n",
    "            }\n",
    "            _save_meta(meta_path, meta)\n",
    "            print(f\"[{symbol}] Saved model → {model_path}\")\n",
    "            print(f\"[{symbol}] Saved meta   → {meta_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[{symbol}] Warning: failed to save artifacts: {e}\")\n",
    "\n",
    "    return actual, pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b4e8b9",
   "metadata": {},
   "source": [
    "### Classifying Stocks as as per ABC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7468dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_stocks_by_return(stock_stats_df):\n",
    "    # Sort by cumulative return descending\n",
    "    sorted_df = stock_stats_df.sort_values(\n",
    "        by=\"cumulative_return\", ascending=False\n",
    "    ).reset_index(drop=True)\n",
    "    n = len(sorted_df)\n",
    "    a_cutoff = int(0.2 * n)\n",
    "    b_cutoff = int(0.5 * n)\n",
    "\n",
    "    categories = [\n",
    "        \"A\" if i < a_cutoff else \"B\" if i < b_cutoff else \"C\" for i in range(n)\n",
    "    ]\n",
    "    sorted_df[\"ABC\"] = categories\n",
    "    return sorted_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39c528a",
   "metadata": {},
   "source": [
    "### Function to help in EOQ calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e857c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_eoq(D, S, H):\n",
    "    return math.sqrt((2 * D * S) / H) if H > 0 else 50\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2feb40ef",
   "metadata": {},
   "source": [
    "### Portfolio Simulation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ba9b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_portfolio(assets_data: dict, abc_map: dict, variant_config: dict):\n",
    "    \"\"\"\n",
    "    assets_data: {symbol: {\"actual\": np.array, \"pred\": np.array, \"vol\": np.array}}\n",
    "    abc_map:     {symbol: \"A\"/\"B\"/\"C\"}\n",
    "    variant_config: {\"use_abc\": bool, \"use_eoq\": bool, \"S\": float, \"H\": 0 or \"vol\", \"tuned\": bool}\n",
    "    \"\"\"\n",
    "    cash = 100000.0\n",
    "    holdings = {symbol: 0 for symbol in assets_data}\n",
    "    total_cost = 0.0\n",
    "    portfolio_values = []\n",
    "\n",
    "    S = float(variant_config.get(\"S\", 10.0))\n",
    "    H = variant_config.get(\"H\", 0)  # 0 or \"vol\"\n",
    "    use_eoq = bool(variant_config.get(\"use_eoq\", False))\n",
    "    use_abc = bool(variant_config.get(\"use_abc\", False))\n",
    "\n",
    "    # ---- timeline sync: truncate to common min length ----\n",
    "    min_len = min(len(v[\"actual\"]) for v in assets_data.values())\n",
    "\n",
    "    for t in range(min_len):\n",
    "        daily_value = cash\n",
    "        for symbol, data in assets_data.items():\n",
    "            price = data[\"actual\"][t]\n",
    "            pred = data[\"pred\"][t]\n",
    "            signal = pred - price\n",
    "\n",
    "            # ABC gating\n",
    "            abc = abc_map.get(symbol, \"B\")\n",
    "            threshold = 0.02 if (use_abc and abc == \"C\") else 0.01\n",
    "            if abs(signal) < threshold:\n",
    "                daily_value += holdings[symbol] * price\n",
    "                continue\n",
    "\n",
    "            # Demand proxy D from forecast magnitude\n",
    "            D = abs(signal) * 100\n",
    "\n",
    "            # Holding cost H_adj: if \"vol\", scale by rolling vol; else pass-through\n",
    "            if use_eoq:\n",
    "                if H == \"vol\":\n",
    "                    vol_t = float(data.get(\"vol\", np.zeros(min_len))[t])\n",
    "                    # scale vol to a per-trade holding cost (simple, consistent proxy)\n",
    "                    H_adj = max(1e-6, 100.0 * vol_t)\n",
    "                else:\n",
    "                    H_adj = float(H)\n",
    "                size = int(calculate_eoq(D, S, H_adj))\n",
    "            else:\n",
    "                size = 80 if abc == \"A\" else 50 if abc == \"B\" else 30\n",
    "\n",
    "            if size <= 0:\n",
    "                daily_value += holdings[symbol] * price\n",
    "                continue\n",
    "\n",
    "            # Execute\n",
    "            if signal > 0 and cash >= size * price + S:  # Buy\n",
    "                holdings[symbol] += size\n",
    "                cash -= size * price + S\n",
    "                total_cost += S\n",
    "            elif signal < 0 and holdings[symbol] >= size:  # Sell\n",
    "                holdings[symbol] -= size\n",
    "                cash += size * price - S\n",
    "                total_cost += S\n",
    "\n",
    "            daily_value += holdings[symbol] * price\n",
    "\n",
    "        portfolio_values.append(daily_value)\n",
    "\n",
    "    return portfolio_values, total_cost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbd158e",
   "metadata": {},
   "source": [
    "### Calcaute the relevant metrics for the portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631decbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_portfolio_metrics(values, forecast_rmse):\n",
    "    values = np.array(values)\n",
    "    returns = np.diff(values) / values[:-1]\n",
    "    cumulative_return = (values[-1] - values[0]) / values[0]\n",
    "    sharpe = np.mean(returns) / np.std(returns) if np.std(returns) else 0\n",
    "    drawdown = np.max(np.maximum.accumulate(values) - values)\n",
    "    max_drawdown = drawdown / np.max(np.maximum.accumulate(values))\n",
    "    return {\n",
    "        \"Cumulative Return\": cumulative_return,\n",
    "        \"Sharpe Ratio\": sharpe,\n",
    "        \"Max Drawdown\": max_drawdown,\n",
    "        \"Forecast RMSE\": forecast_rmse,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32e0311",
   "metadata": {},
   "source": [
    "### Function to run the various Experiments listed in the RP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efe1e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_portfolio_experiments(data_dict: dict, abc_map_seed: dict | None = None):\n",
    "    \"\"\"\n",
    "    data_dict: {symbol: raw_df from load_or_fetch_data()}\n",
    "    Returns: DataFrame summarizing portfolio metrics by configuration and writes CSVs.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    per_asset_rows = []\n",
    "\n",
    "    # === Configs to match paper ===\n",
    "    configs = {\n",
    "        \"LSTM Only\": {\n",
    "            \"use_abc\": False,\n",
    "            \"use_eoq\": False,\n",
    "            \"S\": 10.0,\n",
    "            \"H\": 0.0,\n",
    "            \"tuned\": False,\n",
    "        },\n",
    "        \"LSTM + ABC\": {\n",
    "            \"use_abc\": True,\n",
    "            \"use_eoq\": False,\n",
    "            \"S\": 10.0,\n",
    "            \"H\": 0.0,\n",
    "            \"tuned\": False,\n",
    "        },\n",
    "        \"ABC + EOQ\": {\n",
    "            \"use_abc\": True,\n",
    "            \"use_eoq\": True,\n",
    "            \"S\": 10.0,\n",
    "            \"H\": \"vol\",\n",
    "            \"tuned\": False,\n",
    "        },\n",
    "        \"Tuned ABC + EOQ\": {\n",
    "            \"use_abc\": True,\n",
    "            \"use_eoq\": True,\n",
    "            \"S\": 10.0,\n",
    "            \"H\": \"vol\",\n",
    "            \"tuned\": True,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # === Build forecasts & metrics per asset (last 2 years, ordered split) ===\n",
    "    assets_data = {}\n",
    "    for symbol, raw_df in data_dict.items():\n",
    "        try:\n",
    "            df = last_two_years(raw_df)\n",
    "            df = add_technical_indicators(df)\n",
    "            X_sel, y_target = select_features(df, symbol=symbol)\n",
    "            merged = (\n",
    "                pd.concat([X_sel, y_target], axis=1).dropna().reset_index(drop=True)\n",
    "            )\n",
    "\n",
    "            actual, pred = lstm_forecast_multivariate(\n",
    "                merged, target=\"close\", symbol=symbol\n",
    "            )\n",
    "\n",
    "            # Per-asset metrics\n",
    "            rmse = float(math.sqrt(mean_squared_error(actual, pred)))\n",
    "            mae = float(mean_absolute_error(actual, pred))\n",
    "\n",
    "            # Rolling vol on actual returns (window=20), aligned to test portion length\n",
    "            actual_series = pd.Series(actual)\n",
    "            ret = actual_series.pct_change().fillna(0.0).values\n",
    "            vol = (\n",
    "                pd.Series(ret)\n",
    "                .rolling(20)\n",
    "                .std()\n",
    "                .fillna(method=\"bfill\")\n",
    "                .fillna(0.0)\n",
    "                .values\n",
    "            )\n",
    "\n",
    "            assets_data[symbol] = {\n",
    "                \"actual\": np.asarray(actual),\n",
    "                \"pred\": np.asarray(pred),\n",
    "                \"vol\": vol,\n",
    "            }\n",
    "\n",
    "            per_asset_rows.append(\n",
    "                {\n",
    "                    \"symbol\": symbol,\n",
    "                    \"RMSE\": rmse,\n",
    "                    \"MAE\": mae,\n",
    "                    \"test_len\": len(actual),\n",
    "                }\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"[{symbol}] Skipped due to error: {e}\")\n",
    "\n",
    "    if not assets_data:\n",
    "        raise RuntimeError(\"No assets produced forecasts — aborting.\")\n",
    "\n",
    "    # === ABC map (from per-asset test cumulative return) ===\n",
    "    stock_stats = []\n",
    "    for symbol, v in assets_data.items():\n",
    "        prices = np.asarray(v[\"actual\"])\n",
    "        if len(prices) < 2:\n",
    "            cumret = 0.0\n",
    "        else:\n",
    "            cumret = (prices[-1] - prices[0]) / prices[0]\n",
    "        stock_stats.append({\"symbol\": symbol, \"cumulative_return\": float(cumret)})\n",
    "\n",
    "    stock_stats_df = pd.DataFrame(stock_stats)\n",
    "    classified_df = classify_stocks_by_return(stock_stats_df)\n",
    "    abc_map = dict(zip(classified_df[\"symbol\"], classified_df[\"ABC\"]))\n",
    "    if abc_map_seed:\n",
    "        # allow seed/overrides from outside, if passed\n",
    "        abc_map.update(abc_map_seed)\n",
    "\n",
    "    print(classified_df)\n",
    "\n",
    "    # === Portfolio runs by configuration ===\n",
    "    # for fair comparison, compute average RMSE (not used in trading)\n",
    "    avg_rmse = float(np.mean([r[\"RMSE\"] for r in per_asset_rows]))\n",
    "\n",
    "    for name, config in configs.items():\n",
    "        portfolio_vals, total_cost = simulate_portfolio(\n",
    "            assets_data, abc_map=abc_map, variant_config=config\n",
    "        )\n",
    "        metrics = compute_portfolio_metrics(portfolio_vals, avg_rmse)\n",
    "        metrics[\"Model Variant\"] = name\n",
    "        metrics[\"Total Transaction Cost\"] = total_cost\n",
    "        results.append(metrics)\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    # === Persist outputs ===\n",
    "    Path(\"outputs\").mkdir(exist_ok=True, parents=True)\n",
    "    pd.DataFrame(per_asset_rows).to_csv(\n",
    "        \"outputs/forecast_metrics_by_asset.csv\", index=False\n",
    "    )\n",
    "    results_df.to_csv(\"outputs/portfolio_backtest_summary.csv\", index=False)\n",
    "    classified_df.to_csv(\"outputs/abc_classification.csv\", index=False)\n",
    "\n",
    "    print(\"Saved outputs to outputs/\")\n",
    "\n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed73dcd0",
   "metadata": {},
   "source": [
    "### Running the experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5840a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = load_or_fetch_data(symbols[4], api_key)\n",
    "data_dict = {}\n",
    "for asset in assets:\n",
    "    df = load_or_fetch_data(asset, api_key)\n",
    "    data_dict[asset[\"symbol\"]] = df\n",
    "\n",
    "df.head()  # Display the first few rows of the data\n",
    "\n",
    "final_results_df = run_portfolio_experiments(data_dict)\n",
    "print(final_results_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
